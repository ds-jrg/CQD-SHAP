{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cd6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_atoms(query_type):\n",
    "    \"\"\"Get the number of atoms for a given query type.\"\"\"\n",
    "    atom_mapping = {\n",
    "        '2p': 2, '3p': 3, '2i': 2, '2u': 2, \n",
    "        '3i': 3, 'pi': 3, 'up': 3, 'ip': 3\n",
    "    }\n",
    "    if query_type not in atom_mapping:\n",
    "        raise ValueError(f\"Unsupported query type: {query_type}.\")\n",
    "    return atom_mapping[query_type]\n",
    "\n",
    "def get_query_file_paths(data_dir, query_type, hard=False, split='test'):\n",
    "    \"\"\"Get file paths for query data based on query type.\"\"\"\n",
    "    prefix = split + '_ans_'\n",
    "    file_mapping = {\n",
    "        '2p': prefix + '2c', '3p': prefix + '3c', '2i': prefix + '2i', \n",
    "        '2u': prefix + '2u', '3i': prefix + '3i', 'pi': prefix + 'ci',\n",
    "        'ip': prefix + 'ic', 'up': prefix + 'uc'\n",
    "    }\n",
    "    \n",
    "    if query_type not in file_mapping:\n",
    "        raise ValueError(f\"Unsupported query type: {query_type}.\")\n",
    "    \n",
    "    suffix = '_hard' if hard else ''\n",
    "    filename = f\"{file_mapping[query_type]}{suffix}.pkl\"\n",
    "    return f\"{data_dir}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a05a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import Dataset, Graph\n",
    "from query import  QueryDataset\n",
    "\n",
    "def setup_dataset_and_graphs(data_dir):\n",
    "    \"\"\"Setup dataset and graphs (train, valid, test).\"\"\"\n",
    "    dataset = Dataset()\n",
    "    dataset.set_id2node(f'{data_dir}/ind2ent.pkl')\n",
    "    dataset.set_id2rel(f'{data_dir}/ind2rel.pkl')\n",
    "    dataset.set_node2title(f'{data_dir}/extra/entity2text.txt')\n",
    "\n",
    "    # Setup training graph\n",
    "    graph_train = Graph(dataset)\n",
    "    graph_train.load_triples(f'{data_dir}/train.txt', skip_missing=False, add_reverse=True)\n",
    "    \n",
    "    # Setup validation graph\n",
    "    graph_valid = Graph(dataset)\n",
    "    for edge in graph_train.get_edges():\n",
    "        graph_valid.add_edge(edge.get_head().get_name(), edge.get_name(), edge.get_tail().get_name(), skip_missing=False, add_reverse=False)\n",
    "    graph_valid.load_triples(f'{data_dir}/valid.txt', skip_missing=False, add_reverse=True)\n",
    "    \n",
    "    return dataset, graph_train, graph_valid\n",
    "\n",
    "def load_query_datasets(dataset, data_dir, query_type, split='test'):\n",
    "    \"\"\"Load query datasets for the specific query type.\"\"\"\n",
    "    # Load complete query dataset\n",
    "    query_dataset = QueryDataset(dataset)\n",
    "    query_path = get_query_file_paths(data_dir, query_type, hard=False, split=split)\n",
    "    query_dataset.load_queries_from_pkl(query_path, query_type=query_type)\n",
    "    \n",
    "    # Load hard query dataset\n",
    "    query_dataset_hard = QueryDataset(dataset)\n",
    "    query_path_hard = get_query_file_paths(data_dir, query_type, hard=True, split=split)\n",
    "    query_dataset_hard.load_queries_from_pkl(query_path_hard, query_type=query_type)\n",
    "    \n",
    "    return query_dataset, query_dataset_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5188544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14505 nodes from data/FB15k-237/ind2ent.pkl.\n",
      "Loaded 474 relations from data/FB15k-237/ind2rel.pkl.\n",
      "Loaded 14951 node titles from data/FB15k-237/extra/entity2text.txt.\n"
     ]
    }
   ],
   "source": [
    "dataset, graph_train, graph_valid = setup_dataset_and_graphs('data/FB15k-237')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df3fe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Edge <graph.Edge object at 0x72337e525e40> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337e525e40>, Head ID: 13432, Relation ID: 31\n",
      "Warning: Edge <graph.Edge object at 0x72337e36f700> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337e36f700>, Head ID: 62, Relation ID: 51\n",
      "Warning: Edge <graph.Edge object at 0x72337e193880> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337e193880>, Head ID: 62, Relation ID: 51\n",
      "Warning: Edge <graph.Edge object at 0x72337df659c0> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337df659c0>, Head ID: 62, Relation ID: 51\n",
      "Warning: Edge <graph.Edge object at 0x72337de7c640> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337de7c640>, Head ID: 8009, Relation ID: 363\n",
      "Warning: Edge <graph.Edge object at 0x72337db1e440> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337db1e440>, Head ID: 774, Relation ID: 237\n",
      "Warning: Edge <graph.Edge object at 0x72337db874c0> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337db874c0>, Head ID: 14397, Relation ID: 31\n",
      "Warning: Edge <graph.Edge object at 0x72337da68220> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337da68220>, Head ID: 3387, Relation ID: 430\n",
      "Warning: Edge <graph.Edge object at 0x72337d6156c0> has no tail_id, skipping.\n",
      "Edge: <graph.Edge object at 0x72337d6156c0>, Head ID: 774, Relation ID: 237\n"
     ]
    }
   ],
   "source": [
    "from symbolic_torch import SymbolicReasoning\n",
    "\n",
    "reasoner = SymbolicReasoning(graph_valid, logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f962f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset, query_dataset_hard = load_query_datasets(dataset, 'data/FB15k-237', \"2p\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc52da59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplEx(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(14505, 2000, sparse=True)\n",
      "    (1): Embedding(474, 2000, sparse=True)\n",
      "  )\n",
      ")\n",
      "Successfully loaded model and set to eval mode (device: cuda:0)\n"
     ]
    }
   ],
   "source": [
    "from xcqa_torch import XCQA\n",
    "\n",
    "xcqa = XCQA(symbolic=reasoner, dataset=dataset, logging=False, model_path=\"models/FB15k-237-model-rank-1000-epoch-100-1602508358.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be94bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_type = \"2p\"\n",
    "num_atoms = get_num_atoms(query_type)\n",
    "\n",
    "hard = query_dataset_hard.get_queries(query_type)\n",
    "complete = query_dataset.get_queries(query_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352c932",
   "metadata": {},
   "source": [
    "I have a question regarding the necessary and sufficient evaluation. In the CQDA paper, they mention that they only consider predictions where ‚Äútheir original ùêª@1 and ùëÄùëÖùëÖ are both 1.0‚Äù. You told me to follow the same approach and for each query, to consider the hard answer predicted in the top position.\n",
    "\n",
    "However, in the CQD implementation, when computing metrics such as Hit@1, they remove all easy answers as well as the other hard answers (except for the one currently under evaluation). As a result, it is possible for multiple hard answers in a single query to receive a rank of 1.\n",
    "\n",
    "For example, suppose the model prediction is [4, 78, 3, 2], where 4 is an easy answer and 78 and 3 are hard answers. In this case, both 78 and 3 would have Hit@1 = 1. Would this be acceptable, and do you see any issue with averaging over all such hard answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89c1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(result, answer_complete, target_answer):\n",
    "    \"\"\"\n",
    "    result: pd.Series or pd.DataFrame with index = candidate answers, sorted by predicted score (descending).\n",
    "    answer_complete: set/list/array of all correct answers (to exclude during filtered ranking).\n",
    "    target_answer: the specific answer to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    mrr = 0.0\n",
    "    hit_1 = 0\n",
    "    hit_3 = 0\n",
    "    hit_10 = 0\n",
    "\n",
    "    # Convert to sets for fast exclusion\n",
    "    answer_complete_set = set(answer_complete)\n",
    "\n",
    "    # Get a filtered version of result for each a_hard\n",
    "    result_index = list(result.index)\n",
    "\n",
    "    # Remove all other correct answers from ranking for this answer\n",
    "    filtered_exclude = answer_complete_set - {target_answer}\n",
    "    # Build mask once for speed\n",
    "    filtered_index = [x for x in result_index if x not in filtered_exclude]\n",
    "    if target_answer in filtered_index:\n",
    "        rank = filtered_index.index(target_answer) + 1\n",
    "        mrr += 1.0 / rank\n",
    "        if rank == 1:\n",
    "            hit_1 += 1\n",
    "        if rank <= 3:\n",
    "            hit_3 += 1\n",
    "        if rank <= 10:\n",
    "            hit_10 += 1\n",
    "\n",
    "    return mrr, hit_1, hit_3, hit_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0256b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "cqd_coalition = [1] * num_atoms\n",
    "symbolic_coalition = [0] * num_atoms\n",
    "t_norm = \"prod\"\n",
    "t_conorm = \"prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "877839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger(filename):\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(filename + \"evaluation\")\n",
    "    logger.setLevel(logging.DEBUG)  # Set the lowest level of log messages to capture\n",
    "    \n",
    "    # Formatter to control the log output format\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    \n",
    "    # Console handler (for terminal output)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.DEBUG)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # File handler (for saving logs to a file)\n",
    "    file_handler = logging.FileHandler(filename + \".log\", mode=\"w\")\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Avoid adding multiple handlers if function called multiple times\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(console_handler)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0f4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_dict():\n",
    "    return {\n",
    "        \"mrr\": [],\n",
    "        \"hit_1\": [],\n",
    "        \"hit_3\": [],\n",
    "        \"hit_10\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ffcee",
   "metadata": {},
   "source": [
    "## Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bb80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(\"necessary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56501379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [06:40<00:00, 12.48it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from shapley import shapley_value\n",
    "\n",
    "metrics_cqd = get_metric_dict()\n",
    "metrics_new = get_metric_dict()\n",
    "\n",
    "for i in tqdm(range(len(hard))):\n",
    "    query_hard = hard[i]\n",
    "    query_complete = complete[i]\n",
    "    easy_answers = query_complete.get_answer() \n",
    "    easy_answers = [a for a in easy_answers if a not in query_hard.get_answer()]\n",
    "    cqd_result = xcqa.query_execution(query_hard, k=k, coalition=cqd_coalition, t_norm=t_norm, t_conorm=t_conorm)\n",
    "\n",
    "    current_metrics_cqd = get_metric_dict()\n",
    "    current_metrics_new = get_metric_dict()\n",
    "\n",
    "    for hard_answer in query_hard.answer:\n",
    "        mrr, hit_1, hit_3, hit_10 = compute_metrics(cqd_result, query_complete.answer, hard_answer)\n",
    "        if hit_1 == 1.0 and mrr == 1.0:\n",
    "            #logger.info(f\"Match! Query #{i} and Hard answer {hard_answer}\")\n",
    "            #logger.info(f\"Original metrics: MRR: {mrr}, Hit@1: {hit_1}, Hit@3: {hit_3}, Hit@10: {hit_10}\")\n",
    "            current_metrics_cqd[\"mrr\"].append(mrr)\n",
    "            current_metrics_cqd[\"hit_1\"].append(hit_1)\n",
    "            current_metrics_cqd[\"hit_3\"].append(hit_3)\n",
    "            current_metrics_cqd[\"hit_10\"].append(hit_10)\n",
    "\n",
    "            shapley = {}\n",
    "            for atom in range(num_atoms):\n",
    "                shapley[atom] = shapley_value(xcqa, query_hard, atom, easy_answers, hard_answer, \"rank\", k, t_norm, t_conorm)\n",
    "            \n",
    "            lowest_atom = min(shapley, key=shapley.get)\n",
    "            #logger.info(f\"Lowest contributing atom: {lowest_atom} with Shapley value: {shapley[lowest_atom]} (Full shapley value: {shapley})\")\n",
    "            new_coalition = cqd_coalition.copy()\n",
    "            new_coalition[lowest_atom] = 0\n",
    "            new_result = xcqa.query_execution(query_hard, k=k, coalition=new_coalition, t_norm=t_norm, t_conorm=t_conorm)\n",
    "            mrr_new, hit_1_new, hit_3_new, hit_10_new = compute_metrics(new_result, query_complete.answer, hard_answer)\n",
    "            #logger.info(f\"New metrics after removing atom {lowest_atom}: MRR: {mrr_new}, Hit@1: {hit_1_new}, Hit@3: {hit_3_new}, Hit@10: {hit_10_new}\")\n",
    "            current_metrics_new[\"mrr\"].append(mrr_new)\n",
    "            current_metrics_new[\"hit_1\"].append(hit_1_new)\n",
    "            current_metrics_new[\"hit_3\"].append(hit_3_new)\n",
    "            current_metrics_new[\"hit_10\"].append(hit_10_new)\n",
    "\n",
    "    # if it is not empty\n",
    "    if current_metrics_cqd[\"mrr\"] != []:\n",
    "\n",
    "        for key in current_metrics_new:\n",
    "            current_metrics_new[key] = sum(current_metrics_new[key]) / len(current_metrics_new[key])\n",
    "        for key in current_metrics_cqd:\n",
    "            current_metrics_cqd[key] = sum(current_metrics_cqd[key]) / len(current_metrics_cqd[key])\n",
    "        \n",
    "        metrics_cqd[\"mrr\"].append(current_metrics_cqd[\"mrr\"])\n",
    "        metrics_cqd[\"hit_1\"].append(current_metrics_cqd[\"hit_1\"])\n",
    "        metrics_cqd[\"hit_3\"].append(current_metrics_cqd[\"hit_3\"])\n",
    "        metrics_cqd[\"hit_10\"].append(current_metrics_cqd[\"hit_10\"])\n",
    "        metrics_new[\"mrr\"].append(current_metrics_new[\"mrr\"])\n",
    "        metrics_new[\"hit_1\"].append(current_metrics_new[\"hit_1\"])\n",
    "        metrics_new[\"hit_3\"].append(current_metrics_new[\"hit_3\"])\n",
    "        metrics_new[\"hit_10\"].append(current_metrics_new[\"hit_10\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a5c6386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1549"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics_cqd[\"mrr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db35f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mrr before removing the most important atom: 1.0\n",
      "Average mrr after removing the most important atom: 0.3287072230187944\n",
      "Average mrr difference: -0.6712927769812056\n",
      "Average hit_1 before removing the most important atom: 1.0\n",
      "Average hit_1 after removing the most important atom: 0.2666838963160033\n",
      "Average hit_1 difference: -0.7333161036839967\n",
      "Average hit_3 before removing the most important atom: 1.0\n",
      "Average hit_3 after removing the most important atom: 0.3660907200374906\n",
      "Average hit_3 difference: -0.6339092799625095\n",
      "Average hit_10 before removing the most important atom: 1.0\n",
      "Average hit_10 after removing the most important atom: 0.44883617109698676\n",
      "Average hit_10 difference: -0.5511638289030132\n"
     ]
    }
   ],
   "source": [
    "# report the reduced metrics\n",
    "for key in metrics_new:\n",
    "    avg_before = sum(metrics_cqd[key]) / len(metrics_cqd[key])\n",
    "    avg_after = sum(metrics_new[key]) / len(metrics_new[key])\n",
    "    avg_diff = avg_after - avg_before\n",
    "    print(f\"Average {key} before removing the most important atom: {avg_before}\")\n",
    "    print(f\"Average {key} after removing the most important atom: {avg_after}\")\n",
    "    print(f\"Average {key} difference: {avg_diff}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437eb591",
   "metadata": {},
   "source": [
    "## Sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06333c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(\"sufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8f4e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [40:39<00:00,  2.05it/s]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from shapley import shapley_value\n",
    "\n",
    "metrics_symbolic = get_metric_dict()\n",
    "metrics_new = get_metric_dict()\n",
    "\n",
    "for i in tqdm(range(len(hard))):\n",
    "    query_hard = hard[i]\n",
    "    query_complete = complete[i]\n",
    "    easy_answers = query_complete.get_answer() \n",
    "    easy_answers = [a for a in easy_answers if a not in query_hard.get_answer()]\n",
    "    symbolic_result = xcqa.query_execution(query_hard, k=k, coalition=symbolic_coalition, t_norm=t_norm, t_conorm=t_conorm)\n",
    "\n",
    "    current_metrics_symbolic = get_metric_dict()\n",
    "    current_metrics_new = get_metric_dict()\n",
    "\n",
    "    for hard_answer in query_hard.answer:\n",
    "        #logger.info(f\"Evaluating query # {i} with hard answer: {hard_answer}\")\n",
    "        mrr, hit_1, hit_3, hit_10 = compute_metrics(symbolic_result, query_complete.answer, hard_answer)\n",
    "\n",
    "        if mrr != 1.0 and hit_1 != 1.0:\n",
    "            #logger.info(f\"Original metrics: MRR: {mrr}, Hit@1: {hit_1}, Hit@3: {hit_3}, Hit@10: {hit_10}\")\n",
    "            current_metrics_symbolic[\"mrr\"].append(mrr)\n",
    "            current_metrics_symbolic[\"hit_1\"].append(hit_1)\n",
    "            current_metrics_symbolic[\"hit_3\"].append(hit_3)\n",
    "            current_metrics_symbolic[\"hit_10\"].append(hit_10)\n",
    "\n",
    "            shapley = {}\n",
    "            for atom in range(num_atoms):\n",
    "                shapley[atom] = shapley_value(xcqa, query_hard, atom, easy_answers, hard_answer, \"rank\", k, t_norm, t_conorm)\n",
    "                \n",
    "            lowest_atom = min(shapley, key=shapley.get)\n",
    "            #logger.info(f\"Lowest contributing atom: {lowest_atom} with Shapley value: {shapley[lowest_atom]} (Full shapley value: {shapley})\")\n",
    "\n",
    "            new_coalition = symbolic_coalition.copy()\n",
    "            new_coalition[lowest_atom] = 1\n",
    "\n",
    "            new_result = xcqa.query_execution(query_hard, k=k, coalition=new_coalition, t_norm=t_norm, t_conorm=t_conorm)\n",
    "            mrr_new, hit_1_new, hit_3_new, hit_10_new = compute_metrics(new_result, query_complete.answer, hard_answer)\n",
    "            #logger.info(f\"New metrics after removing atom {lowest_atom}: MRR: {mrr_new}, Hit@1: {hit_1_new}, Hit@3: {hit_3_new}, Hit@10: {hit_10_new}\")\n",
    "            current_metrics_new[\"mrr\"].append(mrr_new)\n",
    "            current_metrics_new[\"hit_1\"].append(hit_1_new)\n",
    "            current_metrics_new[\"hit_3\"].append(hit_3_new)\n",
    "            current_metrics_new[\"hit_10\"].append(hit_10_new)\n",
    "\n",
    "    # if it is not empty\n",
    "    if current_metrics_symbolic[\"mrr\"] != []:\n",
    "\n",
    "        for key in current_metrics_new:\n",
    "            current_metrics_new[key] = sum(current_metrics_new[key]) / len(current_metrics_new[key])\n",
    "        for key in current_metrics_symbolic:\n",
    "            current_metrics_symbolic[key] = sum(current_metrics_symbolic[key]) / len(current_metrics_symbolic[key])\n",
    "        \n",
    "        metrics_symbolic[\"mrr\"].append(current_metrics_symbolic[\"mrr\"])\n",
    "        metrics_symbolic[\"hit_1\"].append(current_metrics_symbolic[\"hit_1\"])\n",
    "        metrics_symbolic[\"hit_3\"].append(current_metrics_symbolic[\"hit_3\"])\n",
    "        metrics_symbolic[\"hit_10\"].append(current_metrics_symbolic[\"hit_10\"])\n",
    "        metrics_new[\"mrr\"].append(current_metrics_new[\"mrr\"])\n",
    "        metrics_new[\"hit_1\"].append(current_metrics_new[\"hit_1\"])\n",
    "        metrics_new[\"hit_3\"].append(current_metrics_new[\"hit_3\"])\n",
    "        metrics_new[\"hit_10\"].append(current_metrics_new[\"hit_10\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d564b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4937"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics_new[\"mrr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af60b5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4937"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics_symbolic[\"mrr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4175869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mrr before removing the most important atom: 0.0011034287627172425\n",
      "Average mrr after removing the most important atom: 0.3200484838822459\n",
      "Average mrr difference: 0.31894505511952864\n",
      "Average hit_1 before removing the most important atom: 0.0\n",
      "Average hit_1 after removing the most important atom: 0.24257409198923327\n",
      "Average hit_1 difference: 0.24257409198923327\n",
      "Average hit_3 before removing the most important atom: 0.0008579196266527068\n",
      "Average hit_3 after removing the most important atom: 0.34149325865913005\n",
      "Average hit_3 difference: 0.34063533903247734\n",
      "Average hit_10 before removing the most important atom: 0.0017161851087366179\n",
      "Average hit_10 after removing the most important atom: 0.4742260084649854\n",
      "Average hit_10 difference: 0.4725098233562488\n"
     ]
    }
   ],
   "source": [
    "# report the reduced metrics\n",
    "for key in metrics_new:\n",
    "    avg_before = sum(metrics_symbolic[key]) / len(metrics_symbolic[key])\n",
    "    avg_after = sum(metrics_new[key]) / len(metrics_new[key])\n",
    "    avg_diff = avg_after - avg_before\n",
    "    print(f\"Average {key} before removing the most important atom: {avg_before}\")\n",
    "    print(f\"Average {key} after removing the most important atom: {avg_after}\")\n",
    "    print(f\"Average {key} difference: {avg_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3e66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
